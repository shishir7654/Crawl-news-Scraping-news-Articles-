{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping News Aritcles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import urllib\n",
    "import requests\n",
    "import bs4\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jagaran Newspaper's official url\n",
    "url = \"https://www.jagran.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to request a link\n",
    "def request_url(link):\n",
    "    \"\"\"\n",
    "    It takes a url and returns the html as string.\n",
    "    \"\"\" \n",
    "    time.sleep(2)\n",
    "    \n",
    "    response = requests.get(link)\n",
    "    html = response.text\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse html\n",
    "def parse_html(to_parse):\n",
    "    \"\"\"\n",
    "    It takes a string, then parse it.\n",
    "    Finally, it retuns a soup object.\n",
    "    \"\"\"\n",
    "    soup = bs4.BeautifulSoup(to_parse, 'html.parser')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to collect all sections links\n",
    "# Like world news, national news ...\n",
    "\n",
    "def all_section(main_url):\n",
    "    \"\"\"\n",
    "    It takes a main url of the newspaper and then\n",
    "    finds almost all the sections in the newspaper.\n",
    "    Finally, it returns the section which we will scrap.\n",
    "    \"\"\"\n",
    "    soup = parse_html(request_url(url))\n",
    "    ul = soup.find(\"div\", class_=\"MainLMenu tab\").ul\n",
    "    section_list = []\n",
    "    for li in ul.find_all(\"li\"):\n",
    "        section_list.append(li.a.get('href'))\n",
    "    # Remove the section which we will not consider\n",
    "    # Like the video section and others\n",
    "    remove = [0, 1, -1, -1, -1]\n",
    "    for i in remove:\n",
    "        section_list.remove(section_list[i])\n",
    "    return section_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All section url \n",
    "section_urls = all_section(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/latest-news.html',\n",
       " '/news/national-news-hindi.html',\n",
       " '/feature-news-hindi.html',\n",
       " '/politics-news-hindi.html',\n",
       " '/world-news-hindi.html',\n",
       " '/common-man-issue-news-hindi.html',\n",
       " '/technology-hindi.html',\n",
       " '/business-hindi.html',\n",
       " '/cricket-hindi.html',\n",
       " '/automobile',\n",
       " '/lifestyle-hindi.html']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "section_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Orientation of the section pages**\n",
    "\n",
    "In this newspaper there are basically two types of layouts: Grid View and List View. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep url of the section which has Grid layout\n",
    "grid_layout_urls = [3, 4, 6, 7, 8, 9]\n",
    "\n",
    "# All Grid half urls\n",
    "grid_urls = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Keep url of the section, which has linear layout(List) view\n",
    "list_layout_urls = [0, 1, 5]\n",
    "\n",
    "# All List half urls\n",
    "list_urls = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting only Gride View section page urls\n",
    "for filter_url in grid_layout_urls:\n",
    "    grid_urls.append(section_urls[filter_url])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting only List View section page urls\n",
    "for filter_url in list_layout_urls:\n",
    "    list_urls.append(section_urls[filter_url])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/politics-news-hindi.html',\n",
       " '/world-news-hindi.html',\n",
       " '/technology-hindi.html',\n",
       " '/business-hindi.html',\n",
       " '/cricket-hindi.html',\n",
       " '/automobile']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "grid_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/latest-news.html',\n",
       " '/news/national-news-hindi.html',\n",
       " '/common-man-issue-news-hindi.html']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking List view urls\n",
    "list_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to build complete url\n",
    "# Returns a string\n",
    "def complete_url(half_url):\n",
    "    \"\"\"\n",
    "    This takes a second half of the url as input\n",
    "    and then it adds the first part of the url.\n",
    "    Finally it returns a complete url.\n",
    "    \"\"\"\n",
    "   # Join the url with the href of world news\n",
    "    full_url = url + half_url\n",
    "    return full_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To store complete urls of the Grid View\n",
    "final_grid_urls =[]\n",
    "\n",
    "# To store complete urls of the List View\n",
    "final_list_urls =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting half urls to complete urls - Grid View\n",
    "for url_get in grid_urls:\n",
    "    final_grid_urls.append(complete_url(url_get))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting half urls to complete urls - List View\n",
    "for url_set in list_urls:\n",
    "    final_list_urls.append(complete_url(url_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.jagran.com/politics-news-hindi.html',\n",
       " 'https://www.jagran.com/world-news-hindi.html',\n",
       " 'https://www.jagran.com/technology-hindi.html',\n",
       " 'https://www.jagran.com/business-hindi.html',\n",
       " 'https://www.jagran.com/cricket-hindi.html',\n",
       " 'https://www.jagran.com/automobile']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the full urls - Grid View\n",
    "final_grid_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.jagran.com/latest-news.html',\n",
       " 'https://www.jagran.com/news/national-news-hindi.html',\n",
       " 'https://www.jagran.com/common-man-issue-news-hindi.html']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the full urls - List View\n",
    "final_list_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for valid urls\n",
    "def valid_url(url):\n",
    "    \"\"\"\n",
    "    Takes an url and checks if the urls is valid.\n",
    "    Returns a boolearn value.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        urllib2.urlopen(url)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def collect(page_urls):\n",
    "    \"\"\"\n",
    "    Takes a list of urls which has a grid view layout,\n",
    "    then it extracts the urls of the articles from it\n",
    "    and then it returns it.\n",
    "    \"\"\"\n",
    "    print(\"Extracting article urls from the following sections:\")\n",
    "    \n",
    "    all_urls = set()\n",
    "    for page_url in page_urls:\n",
    "        print(page_url)\n",
    "        soup_page = parse_html(request_url(page_url))\n",
    "        for div in soup_page.find_all(class_=\"h3\"):\n",
    "            sec_head_href = div.find(\"a\").get(\"href\")\n",
    "            # Checks if the url is valid\n",
    "            # Add only if the url is valid\n",
    "            if valid_url(sec_head_href):\n",
    "                all_urls.add(sec_head_href)\n",
    "            else:\n",
    "                all_urls.add(complete_url(sec_head_href))\n",
    "    return all_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting article urls from the following sections:\n",
      "https://www.jagran.com/politics-news-hindi.html\n",
      "https://www.jagran.com/world-news-hindi.html\n",
      "https://www.jagran.com/technology-hindi.html\n",
      "https://www.jagran.com/business-hindi.html\n",
      "https://www.jagran.com/cricket-hindi.html\n",
      "https://www.jagran.com/automobile\n"
     ]
    }
   ],
   "source": [
    "# Function call to collect all article urls from Grid View Sections\n",
    "all_urls = collect(final_grid_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of uniques aritcles urls\n",
    "# Uncomment the below line to check the length\n",
    "# len(all_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collects the article urls of the List view sections\n",
    "def linear_layout_page(linear_url_list):\n",
    "    \"\"\"\n",
    "    It takes urls of the list view sections and\n",
    "    extracts the article links, then it returns it.\n",
    "    \"\"\"\n",
    "    for url in linear_url_list:\n",
    "        soup = parse_html(request_url(url))\n",
    "        ul = soup.find(\"div\", class_=\"newsFJagran\").ul\n",
    "\n",
    "        for li in ul.find_all(\"li\"):\n",
    "            # Adding to the existing urls from the Grid View sections\n",
    "            all_urls.add(complete_url(li.a.get('href')))\n",
    "        \n",
    "    return all_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funtion call to collect all the article urls - List View Sections\n",
    "all_urls = linear_layout_page(final_list_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count - after adding articles urls from list view sections\n",
    "# len(all_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes list of urls\n",
    "# Returns set with new set of urls\n",
    "def navigator(navigate_url):\n",
    "    \"\"\"\n",
    "    It takes urls of the List View sections and then navigates \n",
    "    to next page till the 10th page, along with it, it also add the page's url\n",
    "    to list. Finally, it returns the list of urls of the pages.\n",
    "    \"\"\"\n",
    "    next_page_url = []\n",
    "    next_page_set = set()\n",
    "    for navigate in navigate_url:\n",
    "        soup = parse_html(request_url(navigate))\n",
    "        url_class =  soup.find(class_=\"last\")\n",
    "        page_nav = url_class.a.get(\"href\")\n",
    "        page_nav = complete_url(page_nav)\n",
    "        next_page_url = page_nav\n",
    "        for _ in range(10):\n",
    "            soup_next = parse_html(request_url(next_page_url))\n",
    "            url_class_next =  soup_next.find(class_=\"last\")\n",
    "            page_nav_next = url_class_next.a.get(\"href\")\n",
    "            page_nav_next = complete_url(page_nav_next)\n",
    "            next_page_url = page_nav_next\n",
    "            next_page_set.add(next_page_url)\n",
    "    return next_page_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function call to collect the links of different pages of the sections - List View sections only\n",
    "aditional_url_set = navigator(final_list_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the length of the adtional urls from the pages of sections\n",
    "# len(aditional_url_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the set into list\n",
    "aditional_url_list = list(aditional_url_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting article lists from the additional urls list\n",
    "all_urls = linear_layout_page(aditional_url_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of all urls\n",
    "# len(all_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To be used for text retivring texts of articles\n",
    "article_urls_list = list(all_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_urls = pd.DataFrame(article_urls_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_final_urls.to_csv(\"final_urls.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract article text\n",
    "\n",
    "def article_text(article_urls):\n",
    "    \"\"\"\n",
    "    It takes article urls list and scrap the \n",
    "    texts from it. Finally, it returns the text\n",
    "    of the articles.\n",
    "    \"\"\"\n",
    "    text = []\n",
    "    for i in article_urls:\n",
    "        article_soup = parse_html(request_url(i))\n",
    "        div = article_soup.find(\"div\", class_=\"articleBody\")\n",
    "        for child_div in div.find_all(\"div\"):\n",
    "            child_div.decompose()\n",
    "        text.append(div.get_text())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(article_urls_list) > 500:\n",
    "    article_text_list = article_text(article_urls_list[0:500])\n",
    "else:\n",
    "    article_text_list = article_text(article_urls_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df = pd.DataFrame(article_text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df.to_csv(\"articles_500.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Atlest 500 aritcle has been extracted from the Jagaran Newspaper with a crawler and exported into a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
